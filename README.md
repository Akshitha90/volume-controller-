This project introduces an innovative way to control audio volume using hand gestures. By leveraging computer vision and machine learning, the system provides a seamless and intuitive interaction method, removing the need for physical controls or voice commands.
The project aims to enhance human-computer interaction by enabling gesture-based volume adjustments. This approach benefits individuals with physical disabilities, improves accessibility, and paves the way for more intuitive smart device interactions.
Technologies Used

Programming Language: Python
Computer Vision: OpenCV, MediaPipe
Automation: PyAutoGUI

How It Works

1. The camera captures hand movements in real-time.
2. The system detects and classifies hand gestures.
3. The corresponding volume adjustment action is triggered.
4. Users receive visual feedback indicating changes.



